{
  "name": "MEX",
  "tagline": "Metadata for Machine Learning Experiments",
  "body": "# Integrating Machine Learning Outputs\r\n\r\n## Applications of [LOG4MEX](http://dne5.com/mex/diagram/log4mex-small.png)\r\n\r\n### FOX\r\n\r\nFOX [13] is a highly accurate open-source framework for named entity recognition.FOX achieves a higher F-measure than state-of-the-art named entity recognition frameworks like Stanford by combining the results of several approaches through ensemble learning technique. \r\nWith respect to the introduced drawbacks,Algorithms and their hyperparameters are set, respectively, by the methods mex.Configuration().add Algorithm(EnumAlgorithm x)and mex.Configuration().Algorithm (EnumAlgorithm x).addPara-meter(id,value). Analogously,mex.Configuration().setTool(Enum-Tools y, version)generates  the  link  to  the  used  version  of  the  software  andmex.Configuration().setModel(name)can  be  used  to  specify  the  classifier. \r\n\r\nFinally, the link to each dataset associated to each execution (or configuration) isset by mex.Configuration().setDataSet(url, name). Instead of trawling through the directory structure (more than 20.000 files in about 5GB of uncompressed data) and make suppositions,SPARQL queries give the answers much easier. \r\n\r\n![Fox architecture](http://www.dne5.com/mex/architecture.png)\r\nFigure 2 depicts an excerpt of code for the generated metadata file. Researchers are able to intuitively understand and absorb the relation of the variables inthe experiment configuration.\r\n\r\n\r\n\r\n**Give me all algorithm which were measured by F-measure.**\r\n\r\n    PREFIX  mexcore:  <http://mex.aksw.org/mex-core#>\r\n    PREFIX  mexperf:  <http://mex.aksw.org/mex-perf#>\r\n    PREFIX  mexalgo:  <http://mex.aksw.org/mex-algo#>\r\n    PREFIX  prov:     <http://www.w3.org/ns/prov#>\r\n    PREFIX  rdfs:     <http://www.w3.org/2000/01/rdf-schema#>\r\n    SELECT DISTINCT ?name ?alg ?perfn ?fMeasure WHERE {\r\n       ?execution prov:used ?alg;\r\n                 prov:id ?name.\r\n       ?perfn prov:wasInformedBy  ?execution.\r\n       ?perfn mexperf:fMeasure ?fMeasure.\r\n       ?alg (rdf:type/rdfs:subClassOf*) mexalgo:NamedAlgorithm.\r\n        } \r\n    ORDER BY DESC (?fMeasure)\r\n    LIMIT 30\r\n\r\nResult: \r\n\r\n\r\n**Give me the best precision achieved by a given algorithm perfomed over dataset called News*.**\r\n\r\n    PREFIX  mexcore:  <http://mex.aksw.org/mex-core#>\r\n    PREFIX  mexperf:  <http://mex.aksw.org/mex-perf#>\r\n    PREFIX  mexalgo:  <http://mex.aksw.org/mex-algo#>\r\n    PREFIX  prov:     <http://www.w3.org/ns/prov#>\r\n    PREFIX  rdfs:     <http://www.w3.org/2000/01/rdf-schema#>\r\n    SELECT DISTINCT ?execution ?algn ?precision WHERE {\r\n      ?execution prov:wasInformedBy ?conf.\r\n      ?conf prov:used ?dataset.\r\n      ?dataset dcterms:title \"News*\".\r\n      ?perfn prov:wasInformedBy  ?execution.\r\n      ?perfn mexperf:precision ?precision.\r\n      ?execution prov:used ?alg.\r\n      ?alg (rdf:type/rdfs:subClassOf*) mexalgo:NamedAlgorithm.\r\n      ?alg rdf:type ?algn.\r\n      } \r\n\r\n    ORDER BY DESC (?precision)\r\n    LIMIT 30\r\n\r\n**Give me the sampling method used by a given execution.**\r\n\r\n    PREFIX  mexcore:  <http://mex.aksw.org/mex-core#>\r\n    PREFIX  mexperf:  <http://mex.aksw.org/mex-perf#>\r\n    PREFIX  mexalgo:  <http://mex.aksw.org/mex-algo#>\r\n    PREFIX  prov:     <http://www.w3.org/ns/prov#>\r\n    PREFIX  rdfs:     <http://www.w3.org/2000/01/rdf-schema#>\r\n    SELECT DISTINCT ?execution ?sampling ?folds ?sequential ?testSize ?trainSize WHERE {\r\n      ?execution prov:wasInformedBy ?conf.\r\n      ?conf prov:used ?sampling.\r\n      ?sampling mexcore:folds ?folds.\r\n      ?sampling mexcore:sequential ?sequential.\r\n      ?sampling mexcore:testSize ?testSize.\r\n      ?sampling mexcore:trainSize ?trainSize.\r\n      } \r\n    ORDER BY ASC (?execution)\r\n    LIMIT 30\r\n\r\n***\r\n\r\n### SML-Bench\r\n***\r\n\r\n### Federated Query Engines\r\n***\r\nA [query](http://mex.aksw.org/mex-core#Execution) that collects [results](http://mex.aksw.org/mex-perf#ExecutionPerformance) from more than one data sources is called **federated query**. The engines that allow executing federated queries are called **federated engines**. In SPARQL endpoint [federation engines evaluation](http://mex.aksw.org/mex-core#Experiment), the [query runtime](http://mex.aksw.org/mex-core#endsAt) is the central [performance measure](http://mex.aksw.org/mex-perf#PerformanceMeasure). In addition, the number of data sources selected, the source selection time, the number of intermediate results, the [precision](http://mex.aksw.org/mex-perf#precision) and [recall](http://mex.aksw.org/mex-perf#recall) are also important measures to pinpoint the limitations of the **federated engines**. \r\n\r\nWe have two type of configurations to run the experiments: 1) we specify the specification ([RAM](http://mex.aksw.org/mex-core#memory), [Hard disk](http://mex.aksw.org/mex-core#hdType), [Processor](http://mex.aksw.org/mex-core#cpu), etc..) of all the machines running the SPARQL endpoints, 2) the specification of the machine running the **federation engines**. In this use case, we exported the [LargeRDFBench] (https://github.com/AKSW/largerdfbench) results in to ``mex`` machine readable format. The selected **federated engines** can be directly compared using simple SPARQL queries over ``mex`` results, optimizing the results analysis process.\r\n\r\n### WASOTA\r\n***\r\n\r\n## More information\r\n\r\n* [MEX Vocabulary](https://github.com/AKSW/mexproject/tree/master/vocabulary)\r\n* [LOG4MEX Library](https://github.com/AKSW/mexproject/tree/master/log4mex)\r\n",
  "note": "Don't delete this file! It's used internally to help with page regeneration."
}